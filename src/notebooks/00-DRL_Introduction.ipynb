{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><strong>Introduction to DRL</strong></h1>\n",
    "\n",
    "| **Chapter** |Â **Title**           | **Subject**                  |\n",
    "| ----------- | ------------------- | ---------------------------- |\n",
    "|  1  | Task                        | Intro to Task                |\n",
    "|     |                             | Episodic vs. Continuous      |\n",
    "|  2  | Reward Hypothesis           | Intro to Reward              |\n",
    "|     |                             | Cummulative Reward           |\n",
    "|     |                             | Discounted Return            |\n",
    "|  3  | Markov Decision Process     | Intro to MDP                 |\n",
    "|     |                             | One-Step Dynamics            |\n",
    "|     |                             | Finite MDP                   |\n",
    "|  4  | Policies                    | Intro to Policy              |\n",
    "|     |                             | State-Value Function         |\n",
    "|     |                             | Bellman Equations            |\n",
    "|     |                             | Optimality                   |\n",
    "|     |                             | Action-Value Function        |\n",
    "|     |                             | Optimal Policies             |\n",
    "|  5  | Monte Carlo Methods         | Intro to MCM                 |\n",
    "|     |                             | Greedy Policies              |\n",
    "|     |                             | Epsilon-Greedy Policies      |\n",
    "|     |                             | Intro to MC Control          |\n",
    "|     |                             | Exploration vs. Exploitation |\n",
    "|     |                             | Incremental Mean             |\n",
    "|     |                             | Constant-Alpha               |\n",
    "|  6  | Temporal-Difference Methods | MC Control Methods           |\n",
    "|     |                             | Sarsa                        |\n",
    "|     |                             | Q-Learning                   |\n",
    "|     |                             | Theory & Practice            |\n",
    "|     |                             | Performance Analysis         |\n",
    "|  7  | RL in Continuous Space      | Intro to Arpan               |\n",
    "|     |                             | Discrete vs. Continuous      |\n",
    "|     |                             | Discretisation               |\n",
    "|     |                             | Tile Coding                  |\n",
    "|     |                             | Coarse Coding                |\n",
    "|     |                             | Function Approx.             |\n",
    "|     |                             | Linear Function Approx.      |\n",
    "|     |                             | Kernel Function              |\n",
    "|     |                             | Non-Linear Function Approx.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1. Episodic vs. Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Reward Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1. Cummulative Reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2. Discounted Return**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Markov Decision Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1. One-Step Dynamics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2. Finite MDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Policies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1. State-Value Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2. Bellman Equations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3. Optimality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4. Action-Value Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5. Optimal Policies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Monte Carlo Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     |                             |               |\n",
    "|     |                             |      |\n",
    "|     |                             |          |\n",
    "|     |                             |  |\n",
    "|     |                             |              |\n",
    "|     |                             | Constant-Alpha               |\n",
    "|     |                             | Constant-Alpha "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1. Greedy Policies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2. Epsilon-Greedy Policies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3. Intro to MC Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4. Exploration vs. Exploitation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.5. Incremental Mean**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.6. Constant-Alpha**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Temporal-Difference Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1. Sarsa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2. Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.3. Theory and Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.4. Performance Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. RL in Continuous Space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1. Intro to Arpam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.2. Discrete vs. Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3. Discretisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.4. Tile Coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.5. Coarse Coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.6. Function Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.7. Linear-Function Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.8. Kernel Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.9. Non-Linear Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. What's Next?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
